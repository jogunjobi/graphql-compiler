{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import namedtuple\n",
    "from itertools import chain, repeat\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    AbstractSet, Any, Callable, Collection, Dict, Generator, Generic, Iterable, List, Mapping,\n",
    "    NamedTuple, Optional, Tuple, TypeVar, Union\n",
    ")\n",
    "\n",
    "from graphql import GraphQLList, GraphQLString, parse\n",
    "from graphql.utils.build_ast_schema import build_ast_schema\n",
    "from graphql_compiler.compiler.compiler_frontend import graphql_to_ir\n",
    "from graphql_compiler.compiler.blocks import (\n",
    "    Backtrack, CoerceType, ConstructResult, Filter, GlobalOperationsStart, MarkLocation, \n",
    "    QueryRoot, Traverse\n",
    ")\n",
    "from graphql_compiler.compiler.compiler_entities import BasicBlock, Expression\n",
    "from graphql_compiler.compiler.expressions import (\n",
    "    BinaryComposition, ContextField, Literal, LocalField, OutputContextField, Variable\n",
    ")\n",
    "from graphql_compiler.compiler.compiler_frontend import IrAndMetadata, graphql_to_ir\n",
    "from graphql_compiler.compiler.helpers import Location, get_only_element_from_collection\n",
    "from graphql_compiler.schema import GraphQLDate, GraphQLDateTime, GraphQLDecimal\n",
    "from graphql_compiler.tests.test_helpers import SCHEMA_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterInfo = namedtuple(\n",
    "    'FilterInfo',\n",
    "    ('field_name', 'op_name', 'value'),\n",
    ")\n",
    "DataToken = TypeVar('DataToken')\n",
    "\n",
    "\n",
    "class DataContext(Generic[DataToken]):\n",
    "    \n",
    "    __slots__ = (\n",
    "        'is_inactive',\n",
    "        'current_token',\n",
    "        'token_at_location',\n",
    "        'expression_stack',\n",
    "    )\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        is_inactive: bool, \n",
    "        current_token: Optional[DataToken], \n",
    "        token_at_location: Dict[Location, Optional[DataToken]], \n",
    "        expression_stack: List[Any],\n",
    "    ):\n",
    "        self.is_inactive = is_inactive\n",
    "        self.current_token = current_token\n",
    "        self.token_at_location = token_at_location\n",
    "        self.expression_stack = expression_stack\n",
    "\n",
    "    @staticmethod\n",
    "    def make_empty_context_from_token(token: DataToken) -> 'DataContext':\n",
    "        return DataContext(False, token, dict(), [])\n",
    "    \n",
    "    def push_value_onto_stack(self, value: Any) -> 'DataContext':\n",
    "        self.expression_stack.append(value)\n",
    "        return self  # for chaining\n",
    "    \n",
    "    def peek_value_on_stack(self) -> Any:\n",
    "        return self.expression_stack[-1]\n",
    "        \n",
    "    def pop_value_from_stack(self) -> Any:\n",
    "        return self.expression_stack.pop()\n",
    "    \n",
    "    def get_context_for_location(self, location: Location) -> 'DataContext':\n",
    "        return DataContext(\n",
    "            False, \n",
    "            self.token_at_location[location], \n",
    "            self.token_at_location, \n",
    "            list(self.expression_stack),\n",
    "        )\n",
    "        \n",
    "\n",
    "class InterpreterAdapter(Generic[DataToken], metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def get_tokens_of_type(\n",
    "        self,\n",
    "        type_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[DataToken]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_property(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        field_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Any]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_neighbors(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        direction: str,\n",
    "        edge_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Iterable[DataToken]]]:\n",
    "        # If using a generator instead of a list for the Iterable[DataToken] part,\n",
    "        # be careful -- generators are not closures! Make sure any state you pull into\n",
    "        # the generator from the outside does not change, or that bug will be hard to find.\n",
    "        # Remember: it's always safer to use a function to produce the generator, since\n",
    "        # that will explicitly preserve all the external values passed into it.\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def can_coerce_to_type(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        type_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, bool]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_operator(operator: str, left_value: Any, right_value: Any) -> Any:\n",
    "    if operator == '=':\n",
    "        return left_value == right_value\n",
    "    elif operator == 'contains':\n",
    "        return right_value in left_value\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _push_values_onto_data_context_stack(\n",
    "    contexts_and_values: Iterable[Tuple[DataContext, Any]]\n",
    ") -> Iterable[DataContext]:\n",
    "    return (\n",
    "        data_context.push_value_onto_stack(value)\n",
    "        for data_context, value in contexts_and_values\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_binary_composition(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Expression,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.left, data_contexts)\n",
    "    )\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.right, data_contexts)\n",
    "    )\n",
    "    \n",
    "    for data_context in data_contexts:\n",
    "        # N.B.: The left sub-expression is evaluated first, therefore its value in the stack\n",
    "        #       is *below* the value of the right sub-expression.\n",
    "        #       These two lines cannot be inlined into the _apply_operator() call since\n",
    "        #       the popping order there would be incorrectly reversed.\n",
    "        right_value = data_context.pop_value_from_stack()\n",
    "        left_value = data_context.pop_value_from_stack()\n",
    "        final_expression_value = _apply_operator(expression.operator, left_value, right_value)\n",
    "        yield (data_context, final_expression_value)\n",
    "\n",
    "        \n",
    "def _evaluate_local_field(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: LocalField,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    field_name = expression.field_name\n",
    "    return adapter.project_property(data_contexts, field_name)\n",
    "\n",
    "\n",
    "def _evaluate_context_field(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Union[ContextField, OutputContextField],\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    location = expression.location.at_vertex()\n",
    "    field_name = expression.location.field\n",
    "    \n",
    "    moved_contexts = (\n",
    "        data_context.get_context_for_location(location).push_value_onto_stack(data_context)\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        (moved_data_context.pop_value_from_stack(), value)\n",
    "        for moved_data_context, value in adapter.project_property(moved_contexts, field_name)\n",
    "    )\n",
    "\n",
    "    \n",
    "def _evaluate_variable(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Variable,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Any]:\n",
    "    variable_value = query_arguments[expression.variable_name[1:]]\n",
    "    return (\n",
    "        (data_context, variable_value)\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Expression,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    type_to_handler = {\n",
    "        BinaryComposition: _evaluate_binary_composition,\n",
    "        ContextField: _evaluate_context_field,\n",
    "        OutputContextField: _evaluate_context_field,\n",
    "        LocalField: _evaluate_local_field,\n",
    "        Variable: _evaluate_variable,\n",
    "    }\n",
    "    expression_type = type(expression)\n",
    "    return type_to_handler[expression_type](adapter, query_arguments, expression, data_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_filter(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Filter,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    predicate = block.predicate\n",
    "    \n",
    "    # TODO(predrag): Handle the \"filters depending on missing optional values pass\" rule.\n",
    "    \n",
    "    yield from (\n",
    "        data_context\n",
    "        for data_context, predicate_value in _evaluate_expression(\n",
    "            adapter, query_arguments, predicate, data_contexts\n",
    "        )\n",
    "        if predicate_value\n",
    "    )\n",
    "    \n",
    "\n",
    "def _handle_traverse(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Traverse,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    if block.optional:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    neighbor_data = adapter.project_neighbors(data_contexts, block.direction, block.edge_name)\n",
    "    for data_context, neighbor_tokens in neighbor_data:\n",
    "        yield from (\n",
    "            # TODO(predrag): Make a helper staticmethod on DataContext for this.\n",
    "            DataContext(\n",
    "                False, neighbor_token, \n",
    "                data_context.token_at_location, list(data_context.expression_stack)\n",
    "            )\n",
    "            for neighbor_token in neighbor_tokens\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _produce_output(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    output_name: str,\n",
    "    output_expression: Expression,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    data_contexts = _print_tap(\n",
    "        'outputting ' + output_name, data_contexts)\n",
    "    \n",
    "    contexts_and_values = _evaluate_expression(\n",
    "        adapter, query_arguments, output_expression, data_contexts)\n",
    "    \n",
    "    for data_context, value in contexts_and_values:\n",
    "        data_context.peek_value_on_stack()[output_name] = value\n",
    "        yield data_context\n",
    "    \n",
    "\n",
    "def _handle_construct_result(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: ConstructResult,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Dict[str, Any]]:\n",
    "    output_fields = block.fields\n",
    "    \n",
    "    data_contexts = (\n",
    "        data_context.push_value_onto_stack(dict())\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "    \n",
    "    for output_name, output_expression in output_fields.items():\n",
    "        data_contexts = _produce_output(\n",
    "            adapter, query_arguments, output_name, output_expression, data_contexts)\n",
    "        \n",
    "    return (\n",
    "        data_context.pop_value_from_stack()\n",
    "        for data_context in data_contexts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_coerce_type(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: CoerceType,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    coercion_type = get_only_element_of_collection(block.target_class)\n",
    "    return (\n",
    "        data_context\n",
    "        for data_context, can_coerce in adapter.can_coerce_to_type(data_contexts, coercion_type)\n",
    "        if can_coerce or data_context.current_token is None\n",
    "    )\n",
    "    \n",
    "\n",
    "def _handle_mark_location(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: MarkLocation,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    current_location = block.location\n",
    "    for data_context in data_contexts:\n",
    "        token_at_location = dict(data_context.token_at_location)\n",
    "        token_at_location[current_location] = data_context.current_token\n",
    "        yield DataContext(\n",
    "            False,  # TODO(predrag): This is almost certainly wrong, revisit and extract into staticmethod.\n",
    "            data_context.current_token,\n",
    "            token_at_location,\n",
    "            list(data_context.expression_stack),\n",
    "        )\n",
    "        \n",
    "\n",
    "def _handle_backtrack(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Backtrack,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    backtrack_location = block.location\n",
    "    for data_context in data_contexts:\n",
    "        yield DataContext(\n",
    "            False,  # TODO(predrag): This is almost certainly wrong, revisit and extract into staticmethod.\n",
    "            data_context.token_at_location[backtrack_location],\n",
    "            data_context.token_at_location,\n",
    "            list(data_context.expression_stack),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_block(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: BasicBlock,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    no_op_types = (GlobalOperationsStart,)\n",
    "    if isinstance(block, no_op_types):\n",
    "        return data_contexts\n",
    "    \n",
    "    data_contexts = _print_tap('pre: ' + str(block), data_contexts)\n",
    "    \n",
    "    handler_functions = {        \n",
    "        CoerceType: _handle_coerce_type,\n",
    "        Filter: _handle_filter,\n",
    "        MarkLocation: _handle_mark_location,\n",
    "        Traverse: _handle_traverse,\n",
    "        Backtrack: _handle_backtrack,\n",
    "    }\n",
    "    return handler_functions[type(block)](adapter, query_arguments, block, data_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_tap(info: str, data_contexts: Iterable[DataContext]) -> Iterable[DataContext]:\n",
    "    return data_contexts\n",
    "#     print('\\n')\n",
    "#     unique_id = hash(info)\n",
    "#     print(unique_id, info)\n",
    "#     from funcy.py3 import chunks\n",
    "#     for context_chunk in chunks(100, data_contexts):\n",
    "#         for context in context_chunk:\n",
    "#             pprint((unique_id, context))\n",
    "#             yield context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_ir(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    ir_and_metadata: IrAndMetadata, \n",
    "    query_arguments: Dict[str, Any]\n",
    ") -> Iterable[Dict[str, Any]]:\n",
    "    ir_blocks = ir_and_metadata.ir_blocks\n",
    "    query_metadata_table = ir_and_metadata.query_metadata_table\n",
    "    \n",
    "    if not ir_blocks:\n",
    "        raise AssertionError()\n",
    "        \n",
    "    first_block = ir_blocks[0]\n",
    "    if not isinstance(first_block, QueryRoot):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    last_block = ir_blocks[-1]\n",
    "    if not isinstance(last_block, ConstructResult):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    middle_blocks = ir_blocks[1:-1]\n",
    "        \n",
    "    start_class = get_only_element_from_collection(first_block.start_class)\n",
    "    \n",
    "    current_data_contexts = (\n",
    "        DataContext.make_empty_context_from_token(token)\n",
    "        for token in adapter.get_tokens_of_type(start_class)\n",
    "    )\n",
    "    \n",
    "    current_data_contexts = _print_tap('starting contexts', current_data_contexts)\n",
    "    \n",
    "    for block in middle_blocks:\n",
    "        current_data_contexts = _handle_block(\n",
    "            adapter, query_arguments, block, current_data_contexts)\n",
    "        \n",
    "    current_data_contexts = _print_tap('ending contexts', current_data_contexts)\n",
    "        \n",
    "    return _handle_construct_result(\n",
    "        adapter, query_arguments, last_block, current_data_contexts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = {\n",
    "    'Animal': [\n",
    "        {'name': 'Scooby Doo', 'uuid': '1001'},\n",
    "        {'name': 'Hedwig', 'uuid': '1002'},\n",
    "        {'name': 'Beethoven', 'uuid': '1003'},\n",
    "        {'name': 'Pongo', 'uuid': '1004'},\n",
    "        {'name': 'Perdy', 'uuid': '1005'},\n",
    "        {'name': 'Dipstick', 'uuid': '1006'},\n",
    "        {'name': 'Dottie', 'uuid': '1007'},\n",
    "        {'name': 'Domino', 'uuid': '1008'},\n",
    "        {'name': 'Little Dipper', 'uuid': '1009'},\n",
    "        {'name': 'Oddball', 'uuid': '1010'},\n",
    "    ],\n",
    "}\n",
    "edges = {\n",
    "    'Animal_ParentOf': [\n",
    "        ('1004', '1006'),\n",
    "        ('1005', '1006'),\n",
    "        ('1006', '1008'),\n",
    "        ('1006', '1009'),\n",
    "        ('1006', '1010'),\n",
    "        ('1007', '1008'),\n",
    "        ('1007', '1009'),\n",
    "        ('1007', '1010'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "vertices_by_uuid = {\n",
    "    vertex['uuid']: vertex\n",
    "    for vertex in chain.from_iterable(vertices.values())\n",
    "}\n",
    "\n",
    "\n",
    "class InMemoryAdapter(InterpreterAdapter[dict]):\n",
    "    def get_tokens_of_type(\n",
    "        self,\n",
    "        type_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[dict]:\n",
    "        return vertices[type_name]\n",
    "\n",
    "    def project_property(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        field_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Any]]:\n",
    "        for data_context in data_contexts:\n",
    "            current_token = data_context.current_token\n",
    "            current_value = current_token[field_name] if current_token is not None else None\n",
    "            yield (data_context, current_value)\n",
    "\n",
    "    def project_neighbors(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        direction: str,\n",
    "        edge_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Iterable[DataToken]]]:\n",
    "        edge_info = edges[edge_name]\n",
    "        \n",
    "        for data_context in data_contexts:\n",
    "            neighbor_tokens = []\n",
    "            current_token = data_context.current_token\n",
    "            if current_token is not None:\n",
    "                uuid = current_token['uuid']\n",
    "                if direction == 'out':\n",
    "                    neighbor_tokens = [\n",
    "                        vertices_by_uuid[destination_uuid]\n",
    "                        for source_uuid, destination_uuid in edge_info\n",
    "                        if source_uuid == uuid\n",
    "                    ]\n",
    "                elif direction == 'in':\n",
    "                    neighbor_tokens = [\n",
    "                        vertices_by_uuid[destination_uuid]\n",
    "                        for source_uuid, destination_uuid in edge_info\n",
    "                        if destination_uuid == uuid\n",
    "                    ]\n",
    "                else:\n",
    "                    raise AssertionError()\n",
    "                \n",
    "            yield (data_context, neighbor_tokens)\n",
    "\n",
    "    def can_coerce_to_type(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        type_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, bool]]:\n",
    "        # TODO(predrag): See if a redesign can make this be a no-op again.\n",
    "        return zip(data_contexts, repeat(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = build_ast_schema(parse(SCHEMA_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"animal_name\")\n",
    "        uuid @output(out_name: \"animal_uuid\")\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"parent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @output(out_name: \"child_name\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"parent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @filter(op_name: \"in_collection\", value: [\"$child_names\"])\n",
    "                 @output(out_name: \"child_name\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {\n",
    "    \"child_names\": ['Domino', 'Dipstick', 'Oddball'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"grandparent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @output(out_name: \"parent_name\")\n",
    "            \n",
    "            out_Animal_ParentOf {\n",
    "                name @output(out_name: \"child_name\")\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parent_name': 'Pongo', 'child_name': 'Dipstick'},\n",
       " {'parent_name': 'Perdy', 'child_name': 'Dipstick'},\n",
       " {'parent_name': 'Dipstick', 'child_name': 'Domino'},\n",
       " {'parent_name': 'Dipstick', 'child_name': 'Oddball'},\n",
       " {'parent_name': 'Dottie', 'child_name': 'Domino'},\n",
       " {'parent_name': 'Dottie', 'child_name': 'Oddball'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_and_metadata = graphql_to_ir(schema, query)\n",
    "result = list(interpret_ir(InMemoryAdapter(), ir_and_metadata, query_arguments))\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
